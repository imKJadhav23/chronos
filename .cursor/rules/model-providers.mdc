---
description: LLM Provider and EmbeddingsProvider implementation contract
globs: engine/model/**/*.go
alwaysApply: false
---

# Model Provider Rules

## Provider (LLM chat)
Interface in `engine/model/provider.go`:
- `Chat(ctx, req *ChatRequest) (*ChatResponse, error)`
- `StreamChat(ctx, req *ChatRequest) (<-chan *ChatResponse, error)`
- `Name() string`, `Model() string`

Use types: `Message`, `ToolCall`, `ChatRequest`, `ChatResponse`, `Usage`, `StopReason`. Constructor pattern: `NewOpenAI(apiKey string) *OpenAI` (or with config struct). Reference: `engine/model/openai.go`.

## EmbeddingsProvider
Interface in `engine/model/embeddings.go`:
- `Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error)`

Use `EmbeddingRequest` (Model, Input []string) and `EmbeddingResponse` (Embeddings [][]float32, Usage). Optional: wrap with `model.NewCachedEmbeddings(provider)` for caching.

## Conventions
- One file per provider: `engine/model/<name>.go`. Package is `model`.
- No init(); expose constructor only. Use `context.Context` for timeouts and cancellation.
- Errors: wrap with `fmt.Errorf("provider name: %w", err)`.
